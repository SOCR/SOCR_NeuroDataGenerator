# content_type <- http_type(response)
# print(content_type)
#
#
# if (content_type == "application/zip" || content_type == "application/x-zip-compressed") {
#   content <- content(response, "raw")
#   writeBin(content, pathToZip)
# } else {
#   stop("Unexpected content type received.")
# }
download.file("https://umich.instructure.com/files/21813670/download?download_frd=1", pathToZip, mode = "wb")
# If you need to start a clean fresh run, remove all old files first! Be careful with this! set eval=T in all R-blocks!
##### First check > list.files("/data/")
##### do.call(file.remove, list(list.files("/data", full.names = TRUE)))
##### unlink("/data/*", recursive=TRUE, force=TRUE)
# library(httr)
pathToZip <- tempfile()
# pathToZip<-paste0(pathToZip,".zip")
#
#
# url <- "https://umich.instructure.com/files/21813670/download?download_frd=1"
# response <- GET(url)
#
# content_type <- http_type(response)
# print(content_type)
#
#
# if (content_type == "application/zip" || content_type == "application/x-zip-compressed") {
#   content <- content(response, "raw")
#   writeBin(content, pathToZip)
# } else {
#   stop("Unexpected content type received.")
# }
download.file("https://umich.instructure.com/files/21813670/download?download_frd=1", pathToZip, mode = "wb")
# If you need to start a clean fresh run, remove all old files first! Be careful with this! set eval=T in all R-blocks!
##### First check > list.files("/data/")
##### do.call(file.remove, list(list.files("/data", full.names = TRUE)))
##### unlink("/data/*", recursive=TRUE, force=TRUE)
# library(httr)
pathToZip <- tempfile()
pathToZip<-paste0(pathToZip,".zip")
#
#
url <- "https://umich.instructure.com/files/21813670/download?download_frd=1"
response <- GET(url)
content_type <- http_type(response)
print(content_type)
if (content_type == "application/zip" || content_type == "application/x-zip-compressed") {
content <- content(response, "raw")
writeBin(content, pathToZip)
} else {
stop("Unexpected content type received.")
}
# download.file("https://umich.instructure.com/files/21813670/download?download_frd=1", pathToZip, mode = "wb")
zip::unzip(pathToZip, files=NULL, exdir = paste0(getwd(),'/data'))
library(tibble)
library(rsample)
train_dir <- "/data/data"
valid_dir <- "/data/mri_valid"
library(magick)   # Needed for TIFF --> PNG image conversion and other image processing tasks
# check if ReadMe file is accessible
# file.rename("/data/ReadMe_TCGA_MRI_Segmentation_Data_Phenotypes.txt", train_dir)
# Import the meta-data
# meta_data <- read.csv(paste0(getwd(),"//data//TCGA_MRI_Segmentation_Data_Phenotypes.csv"))
file_path <- file.path(getwd(), "data", "TCGA_MRI_Segmentation_Data_Phenotypes.csv")
# Read the CSV file
meta_data <- read.csv(file_path)
# note that these are relative file/directory names. To see the complete local path
# tempdir(); getwd()
# Create a validation folder
dir.create(valid_dir)
# Check all n=110 patients are accessible
patients <- list.dirs(train_dir, recursive = FALSE)
length(patients)
# Randomly select 20 Patients for validation, remaining 90=110-20 are for training the DNN model
valid_indices <- sample(1:length(patients), 20)
length(patients)
library(tibble)
library(rsample)
train_dir <- file.path(getwd(),"data","data")
valid_dir <- file.path(getwd(),"data","mri_valid")
library(magick)   # Needed for TIFF --> PNG image conversion and other image processing tasks
# check if ReadMe file is accessible
# file.rename("/data/ReadMe_TCGA_MRI_Segmentation_Data_Phenotypes.txt", train_dir)
# Import the meta-data
# meta_data <- read.csv(paste0(getwd(),"//data//TCGA_MRI_Segmentation_Data_Phenotypes.csv"))
file_path <- file.path(getwd(), "data", "TCGA_MRI_Segmentation_Data_Phenotypes.csv")
# Read the CSV file
meta_data <- read.csv(file_path)
# note that these are relative file/directory names. To see the complete local path
# tempdir(); getwd()
# Create a validation folder
dir.create(valid_dir)
# Check all n=110 patients are accessible
patients <- list.dirs(train_dir, recursive = FALSE)
length(patients)
# Randomly select 20 Patients for validation, remaining 90=110-20 are for training the DNN model
valid_indices <- sample(1:length(patients), 20)
valid_indices
patients[valid_indices] # prints the actual folders where the validation participants' data is
# Extract and Relocate the Validation cases (separate them from training data)
for (i in valid_indices) {
dir.create(file.path(valid_dir, basename(patients[i])))
for (f in list.files(patients[i])) {
file.rename(file.path(train_dir, basename(patients[i]), f), file.path(valid_dir, basename(patients[i]), f))
}
unlink(file.path(train_dir, basename(patients[i])), recursive = TRUE) # clean
}
# Confirm that only 80 patients are left in the standard data folder
# list all training data imaging files: list.dirs(train_dir, recursive = FALSE)
length(list.dirs(train_dir, recursive = FALSE))
# and 30-60 validation cases are in the validation folder
length(list.dirs(valid_dir, recursive = FALSE))
# and check validation data
length(list.files(valid_dir, recursive = T)) # [1] 1268
# Identify the TRAINING and VALIDATION data objects (raw images + tumor masks) as filenames
data_train <- tibble(
img = grep(list.files(train_dir, full.names = TRUE, pattern = "tif", recursive = TRUE),
pattern = 'mask', invert = TRUE, value = TRUE),
mask = grep(list.files(train_dir, full.names = TRUE, pattern = "tif", recursive = TRUE),
pattern = 'mask', value = TRUE)
)
data_valid <- tibble(
img = grep(list.files(valid_dir, full.names = TRUE, pattern = "tif", recursive = TRUE),
pattern = 'mask', invert = TRUE, value = TRUE),
mask = grep(list.files(valid_dir, full.names = TRUE, pattern = "tif", recursive = TRUE),
pattern = 'mask', value = TRUE)
)
# If all training + testing data are in one folder, split them by:
#  data <- initial_split(data_train, prop = 0.8)
# convert all Training Data: TIFF images and masks to PNG format (for easier TF processing downstream)
files_img_tif <- data_train$img[grepl("\\.tif$", data_train$img), drop = TRUE]
data_train_img_png <- lapply(files_img_tif,
function(x) {
# image_write(image_read(x), path = gsub(".tif$", ".png", x), format = "png")
a = image_convert(image_read(x),  format = "png")
image_write(a, path = gsub(".tif$", ".png", x), format = "png")
}
)
files_mask_tif <- data_train$mask[grepl("\\.tif$", data_train$mask), drop = TRUE]
data_train_mask_png <- lapply(files_mask_tif,
function(x) {
# image_write(image_read(x), path = gsub(".tif$", ".png", x), format = "png")
a = image_convert(image_read(x),  format = "png")
image_write(a, path = gsub(".tif$", ".png", x), format = "png")
}
)
gsub(".tif$", ".png", x)
grepl("\\.tif$", data_train$img)
reticulate::use_virtualenv(virtualenv = "r-tensorflow", required = TRUE)
library(keras)
install_keras()
library(reticulate)
install_keras()
py_config()
install.packages("keras")
install.packages("keras")
venv_name <- "r-tensorflow"
reticulate::use_virtualenv(virtualenv = venv_name, required = TRUE)
library(reticulate)
# use_condaenv(condaenv = "pytorch_env", required = TRUE)
# devtools::install_github("rstudio/keras")
library("keras")
# install_keras()
# install.packages("tensorflow")
# remotes::install_github("rstudio/tensorflow")
library(tensorflow)
# install_tensorflow()
# tfaddons::install_tfaddons()
library(tfaddons)
# If necessary, download the U-Net package, before you load it into R
# remotes::install_github("r-tensorflow/unet")
library(tfdatasets)
library(tfds)
library(tfhub)
library(tfruns)
library(torch)
# torch::install_torch()
# remotes::install_github("r-tensorflow/unet")
library(unet)
library(tibble)
# The u-Net call takes additional parameters, e.g., number of downsizing blocks, number of filters to start with,
# number of classes to identify; # ?unet provides details. For instance, we can specify the shape
# of the input images we will be segmenting tumors for: 256*256 3-channel RGB images.
model <- unet(input_shape = c(256, 256, 3))
# to print the model as text output, run:
# model
# Results: # Trainable params: 31,031,745
library(tibble)
library(rsample)
train_dir <- file.path(getwd(),"data","data")
valid_dir <- file.path(getwd(),"data","mri_valid")
library(magick)   # Needed for TIFF --> PNG image conversion and other image processing tasks
# check if ReadMe file is accessible
# file.rename("/data/ReadMe_TCGA_MRI_Segmentation_Data_Phenotypes.txt", train_dir)
# Import the meta-data
# meta_data <- read.csv(paste0(getwd(),"//data//TCGA_MRI_Segmentation_Data_Phenotypes.csv"))
file_path <- file.path(getwd(), "data", "TCGA_MRI_Segmentation_Data_Phenotypes.csv")
# Read the CSV file
meta_data <- read.csv(file_path)
# note that these are relative file/directory names. To see the complete local path
# tempdir(); getwd()
# Create a validation folder
dir.create(valid_dir)
# Check all n=110 patients are accessible
patients <- list.dirs(train_dir, recursive = FALSE)
length(patients)
# Randomly select 20 Patients for validation, remaining 90=110-20 are for training the DNN model
valid_indices <- sample(1:length(patients), 20)
valid_indices
patients[valid_indices] # prints the actual folders where the validation participants' data is
# Extract and Relocate the Validation cases (separate them from training data)
for (i in valid_indices) {
dir.create(file.path(valid_dir, basename(patients[i])))
for (f in list.files(patients[i])) {
file.rename(file.path(train_dir, basename(patients[i]), f), file.path(valid_dir, basename(patients[i]), f))
}
unlink(file.path(train_dir, basename(patients[i])), recursive = TRUE) # clean
}
# Confirm that only 80 patients are left in the standard data folder
# list all training data imaging files: list.dirs(train_dir, recursive = FALSE)
length(list.dirs(train_dir, recursive = FALSE))
# and 30-60 validation cases are in the validation folder
length(list.dirs(valid_dir, recursive = FALSE))
# and check validation data
length(list.files(valid_dir, recursive = T)) # [1] 1268
# Identify the TRAINING and VALIDATION data objects (raw images + tumor masks) as filenames
data_train <- tibble(
img = grep(list.files(train_dir, full.names = TRUE, pattern = "tif", recursive = TRUE),
pattern = 'mask', invert = TRUE, value = TRUE),
mask = grep(list.files(train_dir, full.names = TRUE, pattern = "tif", recursive = TRUE),
pattern = 'mask', value = TRUE)
)
data_valid <- tibble(
img = grep(list.files(valid_dir, full.names = TRUE, pattern = "tif", recursive = TRUE),
pattern = 'mask', invert = TRUE, value = TRUE),
mask = grep(list.files(valid_dir, full.names = TRUE, pattern = "tif", recursive = TRUE),
pattern = 'mask', value = TRUE)
)
print(grepl("\\.tif$", data_train$img))
# If all training + testing data are in one folder, split them by:
#  data <- initial_split(data_train, prop = 0.8)
# convert all Training Data: TIFF images and masks to PNG format (for easier TF processing downstream)
files_img_tif <- data_train$img[grepl("\\.tif$", data_train$img), drop = TRUE]
data_train_img_png <- lapply(files_img_tif,
function(x) {
# image_write(image_read(x), path = gsub(".tif$", ".png", x), format = "png")
a = image_convert(image_read(x),  format = "png")
image_write(a, path = gsub(".tif$", ".png", x), format = "png")
}
)
files_mask_tif <- data_train$mask[grepl("\\.tif$", data_train$mask), drop = TRUE]
data_train_mask_png <- lapply(files_mask_tif,
function(x) {
# image_write(image_read(x), path = gsub(".tif$", ".png", x), format = "png")
a = image_convert(image_read(x),  format = "png")
image_write(a, path = gsub(".tif$", ".png", x), format = "png")
}
)
# Similarly convert all Validation Data
# convert all TIFF images and masks to PNG format (for easier TF processing downstream)
files_valid_img_tif <- data_valid$img[grepl("\\.tif$", data_valid$img), drop = TRUE]
data_valid_img_png <- lapply(files_valid_img_tif,
function(x) {
# image_write(image_read(x), path = gsub(".tif$", ".png", x), format = "png")
a = image_convert(image_read(x),  format = "png")
image_write(a, path = gsub(".tif$", ".png", x), format = "png")
}
)
files_valid_mask_tif <- data_valid$mask[grepl("\\.tif$", data_valid$mask), drop = TRUE]
data_valid_mask_png <- lapply(files_valid_mask_tif,
function(x) {
# image_write(image_read(x), path = gsub(".tif$", ".png", x), format = "png")
a = image_convert(image_read(x),  format = "png")
image_write(a, path = gsub(".tif$", ".png", x), format = "png")
}
)
# Check that the TIF --> PNG conversion worked, inspect one case
head(list.files("/data/data/TCGA_HT_A61A_20000127"))
# data_valid  # check root directory
# Inspect some of the images/masks
# image_info(image_read(data_train_img_png[[3]]))
# image_write(image_read(data_train$img[3]), format = "tiff")
# image_write(image_read(data_train$img[3]), path = paste0(data_train$img[3], ".png"), format = "png")
# a <- image_read(paste0(data_train$img[3], ".png"))
# list.files(train_dir)
# To clean previous file references
# # delete a directory -- must add recursive = TRUE
# unlink("/data", recursive = TRUE); # Clean space # gc(full=T)
# Compute a new binary outcome variable 1=Brain Tumor (mask has at least 1 white pixel), 0=Normal Brain, no white pixels in the mask
pos_neg_diagnosis <- sapply(data_train$mask,
function(x) {   value = max(imager::magick2cimg(image_read(x)))
ifelse (value > 0, 1, 0)  }
)
table(pos_neg_diagnosis)   #; head(data_train)
# pos_neg_diagnosis
#    0    1
# 2046 1103
# Add the normal vs. cancer label to training and testing datasets
data_train$label <- pos_neg_diagnosis
pos_neg_diagnosis_valid <- sapply(data_valid$mask,
function(x) {   value = max(imager::magick2cimg(image_read(x)))
ifelse (value > 0, 1, 0)  }
)
table(pos_neg_diagnosis_valid)
data_valid$label <- pos_neg_diagnosis_valid
# head(data_valid)
# # First check that all brain imaging data is already downloaded and unzipped, if not, see the "Data Import" section above
#
# # train_dir <- "/data/data"
# # valid_dir <- "/data/mri_valid"
# #
# # library(magick)   # Needed for TIFF --> PNG image conversion and other image processing tasks
# #
# # # check if ReadMe file is accessible
# # #file.rename("/data/ReadMe_TCGA_MRI_Segmentation_Data_Phenotypes.txt", train_dir)
# # # Import the meta-data
# # meta_data <- read.csv("/data/TCGA_MRI_Segmentation_Data_Phenotypes.csv")
#
# # note that these are relative file/directory names. To see the complete local path
# # tempdir(); getwd()
#
# # Create a validation folder
# # dir.create(valid_dir)
#
# # Check all n=110 = 90+20 patients are accessible
# patients <- list.dirs(train_dir, recursive = FALSE)
# length(patients)
#
# # Confirm that only 80 patients are left in the standard data folder
# # list all training data imaging files: list.dirs(train_dir, recursive = FALSE)
# length(list.dirs(train_dir, recursive = FALSE))
#
# # and 20-70 validation cases are in the validation folder
# length(list.dirs(valid_dir, recursive = FALSE))
#
# # and check validation data
# length(list.files(valid_dir, recursive = T))
#
# library(tibble)
# # Identify the TRAINING and VALIDATION data objects (raw images + tumor masks) as filenames
# data_train <- tibble(
#   img = grep(list.files(train_dir, full.names = TRUE, pattern = "png", recursive = TRUE),
#         pattern = 'mask', invert = TRUE, value = TRUE),
#   mask = grep(list.files(train_dir, full.names = TRUE, pattern = "png", recursive = TRUE),
#         pattern = 'mask', value = TRUE)
# )
# data_valid <- tibble(
#   img = grep(list.files(valid_dir, full.names = TRUE, pattern = "png", recursive = TRUE),  # or "tif"
#         pattern = 'mask', invert = TRUE, value = TRUE),
#   mask = grep(list.files(valid_dir, full.names = TRUE, pattern = "png", recursive = TRUE),
#         pattern = 'mask', value = TRUE)
# )
#
# # library(rsample)
# #
# # # Compute a new binary outcome variable 1=Brain Tumor (mask has at least 1 white pixel), 0=Normal Brain, no white pixels in the mask
# pos_neg_diagnosis <- sapply(data_train$mask,
#      function(x) {   value = max(imager::magick2cimg(image_read(x)))
#          ifelse (value > 0, 1, 0)  }
#   )
# table(pos_neg_diagnosis)   #; head(data_train)
# # pos_neg_diagnosis
# #    0    1
# # 2046 1103
#
# # Add the normal vs. cancer label to training and testing datasets
# data_train$label <- pos_neg_diagnosis
#
# pos_neg_diagnosis_valid <- sapply(data_valid$mask,
#      function(x) {   value = max(imager::magick2cimg(image_read(x)))
#          ifelse (value > 0, 1, 0)  }
#   )
# table(pos_neg_diagnosis_valid)
# data_valid$label <- pos_neg_diagnosis_valid
# head(data_valid)
library(torch)
library(torchvision)
# data wrangling
library(tidyverse)
library(zeallot)   # needed for the piping function "%<-%" in "brain_dataset()"
# image processing and visualization
library(magick)
#library(cowplot)
# dataset loading
library(pins)
library(zip)
torch_manual_seed(1234)
install_torch()
rlang::last_trace()
install_torch()
install_torch()
install_torch()
install.packages("torch")
library(torch)
library(torchvision)
# data wrangling
library(tidyverse)
library(zeallot)   # needed for the piping function "%<-%" in "brain_dataset()"
# image processing and visualization
library(magick)
#library(cowplot)
# dataset loading
library(pins)
library(zip)
torch_manual_seed(1234)
install_torch()
check_supported_version()
torch::check_supported_version()
torch::check_supported_version()
install_torch()
library(torch)
install_torch()
shiny::runApp('SOCR_ImgGenApp_v1.6')
runApp('SOCR_ImgGenApp_v1.6')
runApp('SOCR_ImgGenApp_v1.6')
runApp('SOCR_ImgGenApp_v1.6')
runApp('SOCR_ImgGenApp_v1.6')
reticulate::py_last_error()
runApp('SOCR_ImgGenApp_v1.6')
runApp('SOCR_ImgGenApp_v1.6')
runApp('SOCR_ImgGenApp_v1.6')
reticulate::py_last_error()
runApp('SOCR_ImgGenApp_v1.6')
runApp('SOCR_ImgGenApp_v1.6')
reticulate::py_last_error()
shiny::runApp('SOCR_ImgGenApp_v1.6')
runApp('SOCR_ImgGenApp_v1.6')
runApp('SOCR_ImgGenApp_v1.6')
library(bslib)
plotlyOutput("plot2d_x")
c()
shiny::runApp('misc/Nifti_plotter')
runApp('misc/Nifti_plotter')
runApp('misc/Nifti_plotter')
x = c(1.0, 2.0, 3.0, 4.0, 1.0, 2.0, 3.0, 4.0, 1.0, 2.0, 3.0, 4.0, 1.0, 2.0, 3.0, 4.0, 1.0, 2.0, 3.0, 4.0, 1.0, 2.0, 3.0, 4.0, 1.0, 2.0, 3.0, 4.0, 1.0, 2.0, 3.0, 4.0, 1.0, 2.0, 3.0, 4.0,)
c(1.0, 2.0, 3.0, 4.0, 1.0, 2.0, 3.0, 4.0, 1.0, 2.0, 3.0, 4.0, 1.0, 2.0, 3.0, 4.0, 1.0, 2.0, 3.0, 4.0, 1.0, 2.0, 3.0, 4.0, 1.0, 2.0, 3.0, 4.0, 1.0, 2.0, 3.0, 4.0, 1.0, 2.0, 3.0, 4.0,)
c(1.0, 2.0, 3.0, 4.0, 1.0, 2.0, 3.0, 4.0, 1.0, 2.0, 3.0, 4.0, 1.0, 2.0,
3.0, 4.0, 1.0, 2.0, 3.0, 4.0, 1.0, 2.0, 3.0, 4.0, 1.0, 2.0, 3.0, 4.0,
1.0, 2.0, 3.0, 4.0, 1.0, 2.0, 3.0, 4.0,)
c(5.0, 5.0, 5.0, 5.0, 6.0, 6.0, 6.0, 6.0, 7.0, 7.0, 7.0, 7.0, 5.0, 5.0, 5.0, 5.0, 6.0, 6.0, 6.0, 6.0, 7.0, 7.0, 7.0, 7.0, 5.0, 5.0, 5.0, 5.0, 6.0, 6.0, 6.0, 6.0, 7.0, 7.0, 7.0, 7.0)
c(12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0)
c(-0.3048106211022167, 0.5806111842123143, -0.8011526357338304, 0.9454451549211168, 0.25382336276203626, -0.49102159389846933, 0.6960584883449115, -0.8555043707508208, 0.7331903200732922, -0.9971732887740798, 0.623012211003653, 0.14984740573347818, 0.8268286794901034, -0.9301059501867618, 0.21945466799406363, 0.6832397038158508, 0.5139784559875352, -0.8817846188147811, 0.9988166912028082, -0.8317914757822045, 0.10598751175115685, -0.21078106590019152, 0.3132001548706699, -0.41209101962194344, 0.7738906815578891, 0.9802396594403116, 0.46771851834275896, -0.3878094208292295, 0.7331903200732922, -0.9971732887740798, 0.623012211003653, 0.14984740573347818, -0.5733818719904229, 0.9395300555699313, -0.9661099892625297, 0.6435150601529656)
c(1.0, 2.0, 3.0, 4.0, 1.0, 2.0, 3.0, 4.0, 1.0, 2.0, 3.0, 4.0, 1.0, 2.0,
3.0, 4.0, 1.0, 2.0, 3.0, 4.0, 1.0, 2.0, 3.0, 4.0, 1.0, 2.0, 3.0, 4.0,
1.0, 2.0, 3.0, 4.0, 1.0, 2.0, 3.0, 4.0)
runApp('misc/Nifti_plotter')
meshgrid(c(1, 2, 3), c(11, 12))
install('pracma')
install.packages("pracma")
meshgrid(c(1, 2, 3), c(11, 12))
library("pracma")
meshgrid(c(1, 2, 3), c(11, 12))
x <- seq(1, 10)
y <- seq(1, 10)
z <- seq(1, 10)
# Create a 3D grid using expand.grid
grid <- expand.grid(x = x, y = y, z = z)
# View the grid
head(grid)
runApp('misc/Nifti_plotter')
runApp('misc/Nifti_plotter')
runApp('misc/Nifti_plotter')
x <- seq(1, 10)
y <- seq(1, 10)
z <- seq(1, 10)
# Create a 3D grid using expand.grid
grid <- expand.grid(x = x, y = y, z = z)
# View the grid
head(grid)
grid$x
View(grid)
View(grid)
View(grid)
View(grid)
runApp('misc/Nifti_plotter')
runApp('misc/Nifti_plotter')
View(grid)
View(grid)
dim(grid$x)
grid$x
dim(grid$x)
length(grid$x)
runApp('misc/Nifti_plotter')
runApp('misc/fileLister')
runApp('misc/fileLister')
runApp('misc/fileLister')
runApp('misc/fileLister')
runApp('SOCR_ImgGenApp_v1.6')
runApp('SOCR_ImgGenApp_v1.6')
runApp('SOCR_ImgGenApp_v1.6')
runApp('SOCR_ImgGenApp_v1.6')
runApp('SOCR_ImgGenApp_v1.6')
runApp('SOCR_ImgGenApp_v1.6')
runApp('SOCR_ImgGenApp_v1.6')
runApp('SOCR_ImgGenApp_v1.6')
runApp('SOCR_ImgGenApp_v1.6')
runApp('SOCR_ImgGenApp_v1.6')
runApp('SOCR_ImgGenApp_v1.6')
runApp('SOCR_ImgGenApp_v1.6')
runApp('SOCR_ImgGenApp_v1.6')
runApp('SOCR_ImgGenApp_v1.6')
runApp('SOCR_ImgGenApp_v1.6')
runApp('SOCR_ImgGenApp_v1.6')
runApp('SOCR_ImgGenApp_v1.6')
runApp('SOCR_ImgGenApp_v1.6')
runApp('SOCR_ImgGenApp_v1.6')
runApp('SOCR_ImgGenApp_v1.6')
runApp('SOCR_ImgGenApp_v1.6')
runApp('SOCR_ImgGenApp_v1.6')
shiny::runApp('SOCR_ImgGenApp_v1.6')
runApp('SOCR_ImgGenApp_v1.6')
